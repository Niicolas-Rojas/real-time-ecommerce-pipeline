
# ğŸ“¦ Real-Time E-commerce Pipeline

Este proyecto simula un sistema de procesamiento de datos para un sitio de e-commerce utilizando un pipeline hÃ­brido (batch + streaming), desplegado sobre Google Cloud Platform con herramientas modernas como Apache Airflow y Apache Kafka.

---

## ğŸš€ TecnologÃ­as Utilizadas

- **Google Cloud Platform (GCP)**
  - Cloud Storage: almacenamiento tipo Data Lake
  - BigQuery: almacenamiento analÃ­tico
- **Apache Airflow**: orquestaciÃ³n de pipelines batch
- **Apache Kafka**: procesamiento de eventos en tiempo real
- **Python 3.10**
- LibrerÃ­as: `kafka-python`, `gcsfs`, `google-cloud-bigquery`
- (Opcional) Docker para contenerizaciÃ³n

---

## ğŸ“ Estructura del Proyecto

```
real-time-ecommerce-pipeline/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                 # Datos originales (JSON, CSV)
â”‚   â””â”€â”€ processed/           # Datos transformados listos para anÃ¡lisis
â”‚
â”œâ”€â”€ kafka/
â”‚   â”œâ”€â”€ producer/            # Produce eventos desde JSON a Kafka
â”‚   â””â”€â”€ consumer/            # Consume eventos y los escribe en BigQuery
â”‚
â”œâ”€â”€ dags/                    # DAGs de Airflow para procesamiento batch
â”œâ”€â”€ scripts/                 # Scripts para setup de infraestructura
â”œâ”€â”€ airflow/gcp-creds/       # Clave de servicio GCP (JSON)
â”œâ”€â”€ main.py                  # AutomatizaciÃ³n de datasets y tablas
â””â”€â”€ requirements.txt         # Dependencias del entorno
```

---

## ğŸ” Pipeline Batch

1. Subida de archivos `raw/` a GCS con Airflow.
2. TransformaciÃ³n de datos y guardado en `processed/`.
3. Carga de archivos CSV transformados a BigQuery.

---

## âš¡ Pipeline Streaming

1. Un producer Kafka lee eventos `user_events.json` desde GCS.
2. Publica los eventos al topic `user-events`.
3. Un consumer Kafka los inserta en `user_events_streaming` de BigQuery:
   - Valida campos requeridos
   - Detecta duplicados antes de insertar

---

## ğŸ“Š Tablas en BigQuery

- `customers`
- `products`
- `transactions`
- `user_events` (batch)
- `user_events_streaming` (streaming)

---

## âœ… Validaciones Implementadas

- ValidaciÃ³n de campos vacÃ­os
- PrevenciÃ³n de duplicados por `event_id`
- Reporte de errores en inserciÃ³n

---

## ğŸ› ï¸ CÃ³mo Ejecutar

1. **Pre-requisitos**:
   - AutenticaciÃ³n GCP vÃ­a JSON (`GOOGLE_APPLICATION_CREDENTIALS`)
   - Kafka corriendo en `localhost:9092`

2. **InstalaciÃ³n**:
   ```bash
   pip install -r requirements.txt
   ```

3. **Infraestructura**:
   ```bash
   python main.py
   python scripts/create_streaming_dataset.py
   ```

4. **Airflow**:
   - Ejecuta manualmente los DAGs desde la UI

5. **Streaming**:
   ```bash
   python kafka/producer/kafka_producer.py
   python kafka/consumer/kafka_consumer.py
   ```

---

## ğŸ“Œ Autor

Proyecto desarrollado por **NicolÃ¡s Rojas DÃ­az** como simulaciÃ³n profesional de un sistema de procesamiento de datos moderno.  
**Finalizado:** 03-07-2025
